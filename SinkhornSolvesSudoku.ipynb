{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "atomic-hunger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix:\n",
      " [[0. 9. 6. 3. 8.]\n",
      " [5. 6. 1. 1. 5.]\n",
      " [9. 8. 4. 8. 1.]\n",
      " [0. 3. 0. 4. 4.]\n",
      " [4. 4. 7. 6. 3.]]\n",
      "\n",
      " Doubly stochastic matrix:\n",
      " [[0.         0.25679466 0.35602847 0.11239826 0.27477861]\n",
      " [0.39528436 0.23542506 0.08160024 0.05152242 0.23616792]\n",
      " [0.39283441 0.17330808 0.18020997 0.2275693  0.02607824]\n",
      " [0.         0.22957702 0.         0.40194071 0.36848227]\n",
      " [0.21149306 0.10496824 0.38201989 0.2067493  0.09476951]]\n",
      "Row sums: [1. 1. 1. 1. 1.]\n",
      "Column sums: [0.99961182 1.00007306 0.99985856 1.00017999 1.00027656]\n"
     ]
    }
   ],
   "source": [
    "#LeetArxiv implementation of the paper: Sinkhorn Solves Sudoku\n",
    "#Full walkthrough: https://leetarxiv.substack.com/p/sinkhorn-solves-sudoku\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\"\"\"Ensure input matrix is square, 2D, non-negative, and has total support.\"\"\"\n",
    "\"\"\"The C version we wrote here has better checking:https://leetarxiv.substack.com/p/sinkhorn-solves-sudoku \"\"\"\n",
    "def CheckMatrix(P):\n",
    "    P = np.asarray(P)\n",
    "    assert np.all(P >= 0)\n",
    "    assert P.ndim == 2\n",
    "    assert P.shape[0] == P.shape[1]\n",
    "\n",
    "    if not np.all(P.T.dot(np.ones((P.shape[0], 1))) != 0) or not np.all(P.dot(np.ones((P.shape[0], 1))) != 0):\n",
    "        warnings.warn(\"Matrix P must have total support.\", UserWarning)\n",
    "    return P\n",
    "\n",
    "\n",
    "def _normalize(P, max_iter=1000, epsilon=1e-3):\n",
    "    N = P.shape[0]\n",
    "    r = np.ones((N, 1))\n",
    "    c = 1 / (P.T @ r)\n",
    "    r = 1 / (P @ c)\n",
    "\n",
    "    def stopping_criteria(P_eps):\n",
    "        row_sums = np.sum(P_eps, axis=1)\n",
    "        col_sums = np.sum(P_eps, axis=0)\n",
    "        return (\n",
    "            np.all((1 - epsilon) <= row_sums) and np.all(row_sums <= (1 + epsilon)) and\n",
    "            np.all((1 - epsilon) <= col_sums) and np.all(col_sums <= (1 + epsilon))\n",
    "        )\n",
    "\n",
    "    def iterate(P, r, c, iteration=0):\n",
    "        D1 = np.diag(np.squeeze(r))\n",
    "        D2 = np.diag(np.squeeze(c))\n",
    "        P_eps = D1 @ P @ D2\n",
    "\n",
    "        if stopping_criteria(P_eps):\n",
    "            return P_eps, D1, D2, iteration, \"epsilon\"\n",
    "        if iteration >= max_iter:\n",
    "            return P_eps, D1, D2, iteration, \"max_iter\"\n",
    "\n",
    "        c_new = 1 / (P.T @ r)\n",
    "        r_new = 1 / (P @ c_new)\n",
    "        return iterate(P, r_new, c_new, iteration + 1)\n",
    "\n",
    "    return iterate(P, r, c)\n",
    "\n",
    "\n",
    "def sinkhorn_knopp(P, max_iter=1000, epsilon=1e-3):\n",
    "    assert isinstance(max_iter, (int, float)) and max_iter > 0\n",
    "    assert isinstance(epsilon, (int, float)) and 0 < epsilon < 1\n",
    "\n",
    "    P = CheckMatrix(P)\n",
    "    P_ds, D1, D2, iterations, stopping_condition = _normalize(P, int(max_iter), epsilon)\n",
    "\n",
    "    return P_ds, {\n",
    "        \"iterations\": iterations,\n",
    "        \"stopping_condition\": stopping_condition,\n",
    "        \"D1\": D1,\n",
    "        \"D2\": D2\n",
    "    }\n",
    "\n",
    "def test_sinkhorn_knopp():\n",
    "    P = np.array([\n",
    "    [0.000, 9.000, 6.000, 3.000, 8.000],\n",
    "    [5.000, 6.000, 1.000, 1.000, 5.000],\n",
    "    [9.000, 8.000, 4.000, 8.000, 1.000],\n",
    "    [0.000, 3.000, 0.000, 4.000, 4.000],\n",
    "    [4.000, 4.000, 7.000, 6.000, 3.000]\n",
    "])\n",
    "\n",
    "    print(\"Original Matrix:\\n\", P)\n",
    "\n",
    "    # Functional Version\n",
    "    P_ds_func, meta = sinkhorn_knopp(P)\n",
    "    print(\"\\n Doubly stochastic matrix:\\n\", P_ds_func)\n",
    "    print(\"Row sums:\", np.sum(P_ds_func, axis=1))\n",
    "    print(\"Column sums:\", np.sum(P_ds_func, axis=0))\n",
    "\n",
    "test_sinkhorn_knopp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f878f8c",
   "metadata": {},
   "source": [
    "# Step 2: Download Sudoku Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reported-refrigerator",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2906.11it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d760459d43c437cb93d0bbc2c9982c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv:   0%|          | 0.00/79.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 422786/422786 [00:00<00:00, 791739.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pydantic\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import hf_hub_download\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Global list mapping each dihedral transform id to its inverse.\n",
    "# Index corresponds to the original tid, and the value is its inverse.\n",
    "DIHEDRAL_INVERSE = [0, 3, 2, 1, 4, 5, 6, 7]\n",
    "\n",
    "\n",
    "class PuzzleDatasetMetadata(pydantic.BaseModel):\n",
    "    pad_id: int\n",
    "    ignore_label_id: Optional[int]\n",
    "    blank_identifier_id: int\n",
    "    vocab_size: int\n",
    "    seq_len: int\n",
    "    num_puzzle_identifiers: int\n",
    "    total_groups: int\n",
    "    mean_puzzle_examples: float\n",
    "    total_puzzles: int\n",
    "    sets: List[str]\n",
    "\n",
    "\n",
    "def dihedral_transform(arr: np.ndarray, tid: int) -> np.ndarray:\n",
    "    \"\"\"8 dihedral symmetries by rotate, flip and mirror\"\"\"\n",
    "    \n",
    "    if tid == 0:\n",
    "        return arr  # identity\n",
    "    elif tid == 1:\n",
    "        return np.rot90(arr, k=1)\n",
    "    elif tid == 2:\n",
    "        return np.rot90(arr, k=2)\n",
    "    elif tid == 3:\n",
    "        return np.rot90(arr, k=3)\n",
    "    elif tid == 4:\n",
    "        return np.fliplr(arr)       # horizontal flip\n",
    "    elif tid == 5:\n",
    "        return np.flipud(arr)       # vertical flip\n",
    "    elif tid == 6:\n",
    "        return arr.T                # transpose (reflection along main diagonal)\n",
    "    elif tid == 7:\n",
    "        return np.fliplr(np.rot90(arr, k=1))  # anti-diagonal reflection\n",
    "    else:\n",
    "        return arr\n",
    "    \n",
    "    \n",
    "def inverse_dihedral_transform(arr: np.ndarray, tid: int) -> np.ndarray:\n",
    "    return dihedral_transform(arr, DIHEDRAL_INVERSE[tid])\n",
    "\n",
    "\n",
    "class DataProcessConfig(BaseModel):\n",
    "    source_repo: str = \"sapientinc/sudoku-extreme\"\n",
    "    output_dir: str = \"data/sudoku-extreme-full\"\n",
    "\n",
    "    subsample_size: Optional[int] = None\n",
    "    min_difficulty: Optional[int] = None\n",
    "    num_aug: int = 0\n",
    "\n",
    "\n",
    "def shuffle_sudoku(board: np.ndarray, solution: np.ndarray):\n",
    "    # Create a random digit mapping: a permutation of 1..9, with zero (blank) unchanged\n",
    "    digit_map = np.pad(np.random.permutation(np.arange(1, 10)), (1, 0))\n",
    "    \n",
    "    # Randomly decide whether to transpose.\n",
    "    transpose_flag = np.random.rand() < 0.5\n",
    "\n",
    "    # Generate a valid row permutation:\n",
    "    # - Shuffle the 3 bands (each band = 3 rows) and for each band, shuffle its 3 rows.\n",
    "    bands = np.random.permutation(3)\n",
    "    row_perm = np.concatenate([b * 3 + np.random.permutation(3) for b in bands])\n",
    "\n",
    "    # Similarly for columns (stacks).\n",
    "    stacks = np.random.permutation(3)\n",
    "    col_perm = np.concatenate([s * 3 + np.random.permutation(3) for s in stacks])\n",
    "\n",
    "    # Build an 81->81 mapping. For each new cell at (i, j)\n",
    "    # (row index = i // 9, col index = i % 9),\n",
    "    # its value comes from old row = row_perm[i//9] and old col = col_perm[i%9].\n",
    "    mapping = np.array([row_perm[i // 9] * 9 + col_perm[i % 9] for i in range(81)])\n",
    "\n",
    "    def apply_transformation(x: np.ndarray) -> np.ndarray:\n",
    "        # Apply transpose flag\n",
    "        if transpose_flag:\n",
    "            x = x.T\n",
    "        # Apply the position mapping.\n",
    "        new_board = x.flatten()[mapping].reshape(9, 9).copy()\n",
    "        # Apply digit mapping\n",
    "        return digit_map[new_board]\n",
    "\n",
    "    return apply_transformation(board), apply_transformation(solution)\n",
    "\n",
    "\n",
    "def convert_subset(set_name: str, config: DataProcessConfig):\n",
    "    # Read CSV\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(hf_hub_download(config.source_repo, f\"{set_name}.csv\", repo_type=\"dataset\"), newline=\"\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)  # Skip header\n",
    "        for source, q, a, rating in reader:\n",
    "            if (config.min_difficulty is None) or (int(rating) >= config.min_difficulty):\n",
    "                assert len(q) == 81 and len(a) == 81\n",
    "                \n",
    "                inputs.append(np.frombuffer(q.replace('.', '0').encode(), dtype=np.uint8).reshape(9, 9) - ord('0'))\n",
    "                labels.append(np.frombuffer(a.encode(), dtype=np.uint8).reshape(9, 9) - ord('0'))\n",
    "\n",
    "    # If subsample_size is specified for the training set,\n",
    "    # randomly sample the desired number of examples.\n",
    "    if set_name == \"train\" and config.subsample_size is not None:\n",
    "        total_samples = len(inputs)\n",
    "        if config.subsample_size < total_samples:\n",
    "            indices = np.random.choice(total_samples, size=config.subsample_size, replace=False)\n",
    "            inputs = [inputs[i] for i in indices]\n",
    "            labels = [labels[i] for i in indices]\n",
    "\n",
    "    # Generate dataset\n",
    "    num_augments = config.num_aug if set_name == \"train\" else 0\n",
    "\n",
    "    results = {k: [] for k in [\"inputs\", \"labels\", \"puzzle_identifiers\", \"puzzle_indices\", \"group_indices\"]}\n",
    "    puzzle_id = 0\n",
    "    example_id = 0\n",
    "    \n",
    "    results[\"puzzle_indices\"].append(0)\n",
    "    results[\"group_indices\"].append(0)\n",
    "    \n",
    "    for orig_inp, orig_out in zip(tqdm(inputs), labels):\n",
    "        for aug_idx in range(1 + num_augments):\n",
    "            # First index is not augmented\n",
    "            if aug_idx == 0:\n",
    "                inp, out = orig_inp, orig_out\n",
    "            else:\n",
    "                inp, out = shuffle_sudoku(orig_inp, orig_out)\n",
    "\n",
    "            # Push puzzle (only single example)\n",
    "            results[\"inputs\"].append(inp)\n",
    "            results[\"labels\"].append(out)\n",
    "            example_id += 1\n",
    "            puzzle_id += 1\n",
    "            \n",
    "            results[\"puzzle_indices\"].append(example_id)\n",
    "            results[\"puzzle_identifiers\"].append(0)\n",
    "            \n",
    "        # Push group\n",
    "        results[\"group_indices\"].append(puzzle_id)\n",
    "        \n",
    "    # To Numpy\n",
    "    def _seq_to_numpy(seq):\n",
    "        arr = np.concatenate(seq).reshape(len(seq), -1)\n",
    "        \n",
    "        assert np.all((arr >= 0) & (arr <= 9))\n",
    "        return arr + 1\n",
    "    \n",
    "    results = {\n",
    "        \"inputs\": _seq_to_numpy(results[\"inputs\"]),\n",
    "        \"labels\": _seq_to_numpy(results[\"labels\"]),\n",
    "        \n",
    "        \"group_indices\": np.array(results[\"group_indices\"], dtype=np.int32),\n",
    "        \"puzzle_indices\": np.array(results[\"puzzle_indices\"], dtype=np.int32),\n",
    "        \"puzzle_identifiers\": np.array(results[\"puzzle_identifiers\"], dtype=np.int32),\n",
    "    }\n",
    "\n",
    "    # Metadata\n",
    "    metadata = PuzzleDatasetMetadata(\n",
    "        seq_len=81,\n",
    "        vocab_size=10 + 1,  # PAD + \"0\" ... \"9\"\n",
    "        pad_id=0,\n",
    "        ignore_label_id=0,\n",
    "        blank_identifier_id=0,\n",
    "        num_puzzle_identifiers=1,\n",
    "        total_groups=len(results[\"group_indices\"]) - 1,\n",
    "        mean_puzzle_examples=1,\n",
    "        total_puzzles=len(results[\"group_indices\"]) - 1,\n",
    "        sets=[\"all\"]\n",
    "    )\n",
    "\n",
    "    # Save metadata as JSON.\n",
    "    save_dir = os.path.join(config.output_dir, set_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(save_dir, \"dataset.json\"), \"w\") as f:\n",
    "        json.dump(metadata.dict(), f)\n",
    "        \n",
    "    # Save data\n",
    "    for k, v in results.items():\n",
    "        np.save(os.path.join(save_dir, f\"all__{k}.npy\"), v)\n",
    "        \n",
    "    # Save IDs mapping (for visualization only)\n",
    "    with open(os.path.join(config.output_dir, \"identifiers.json\"), \"w\") as f:\n",
    "        json.dump([\"<blank>\"], f)\n",
    "\n",
    "\n",
    "config = DataProcessConfig(\n",
    "    source_repo=\"sapientinc/sudoku-extreme\", \n",
    "    output_dir=\"sudoku-data\",        \n",
    "    subsample_size=1000,                      \n",
    "    min_difficulty=None,                      \n",
    "    num_aug=2                                 \n",
    ")\n",
    "\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "convert_subset(\"train\", config)\n",
    "convert_subset(\"test\", config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
